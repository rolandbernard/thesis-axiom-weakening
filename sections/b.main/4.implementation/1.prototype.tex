
The implementation of the presented axiom weakening operator is based on the implementation at \cite{ontologyutils} for weakening in \ALC as discussed in \cite{troquard2018repairing}. The implementation has been significantly extended using the operators and algorithms presented in this thesis. We will now give a brief summary covering some implementation details.

The implementation has been made such that different algorithms can be used for repairing the ontologies. Further, for the repairs based on axiom weakening, the approaches for selection the reference ontology or the bad axioms are easily configurable. Further, the implementation contains some additional flags that can be set to modify the behaviour of the refinement and axiom weakening operators. For example, the caching explained in \cref{cache-impl} can optionally be disabled. Some other flags have been added to strictly ensure only axioms that conform to \ALC, \SROIQ, or the negation normal form of one of them are accepted and produced. Further, refinement of roles may optionally be disabled, used as defined in \cref{def:covers}, or can even be extended to use non-simple roles in contexts where they are allowed. In addition to the approach to weakening RIAs shown in \cref{def:weaken}, the alternative approach using a fixed preorder discussed in \cref{rbox-alternative} may be selected by means of a flag to the axiom weakening operator, allowing for more flexible weakening of RIAs. Note that the flags have been set up for the evaluation in \cref{evaluation} such that the weakening conforms to the definitions in \cref{weakening-sroiq}, and to accept only \SROIQ axioms.

While the implementation features the extensions to the weakening operator discussed in \cref{weakening-owl-2-dl}, to enable the evaluation of the proposed version of axiom weakening operating only on \SROIQ concepts and axioms, the ontologies had to be normalized. To achieve this, axioms and concepts had to be modified. For TBox axioms, the OWL API is already able to do most of the work by providing a method that converts axioms into one or more GCIs. For ABox axioms, the only transformation that had to be done is converting $n$-ary same individual and different individuals axioms to $(n - 1) n$ different binary equalities and inequalities. In the case of same individual axioms, there is also a flag that will cause the normalization to generate only $n - 1$ equalities, such that one of the individuals is asserted to be equal to all the others. The same mechanisms have been implemented for RBox axioms asserting equivalence or disjointness between roles. The rest of the RBox axioms have been transformed into RIAs and GCIs. There are only two kinds of concepts that had to be specially handles. The exact cardinality constraint, which is transformed into a conjunction between an at-least and an at-most constraint.

\begin{algorithm}[ht]
  \begin{algorithmic}
    \State $\Omcfull \gets \Omc$
    \State $\Omcref \gets \textsc{FindConsistentSubset(\Omc)}$
    \While{$\Omc$ is inconsistent}
    \State $\phi_\textnormal{bad} \gets \textsc{FindBadAxiom}(\Omc)$
    \State $\phi_\textnormal{weaker} \gets \textsc{SelectWeakerAxiom}(g_{\Omcref,\Omcfull}(\phi_\textnormal{bad}))$
    \State $\Omc \gets (\Omc \setminus \{\phi_\textnormal{bad}\}) \cup \{\phi_\textnormal{weaker}\}$
    \EndWhile
    \State Return $\Omc$
  \end{algorithmic}
  \caption{\textsc{RepairOntologyWeaken}($\Omc$)}
  \label{algo:repair-weaken}
\end{algorithm}

The automatic repair algorithm using axiom weakening has implemented as depicted in \cref{algo:repair-weaken}. It repeatedly selects ``bad axioms'' and replaces them with weaker axioms generated by means of the axiom weakening operator defined in \cref{def:weaken}. This is repeated until the ontology becomes consistent. In the implementation used for the evaluation in this thesis, the reference ontology has been selected by randomly sampling a maximal consistent subset. The prototype does, however, also include multiple alternative implementations of $\textsc{FindConsistentSubset}$. One is to choose as reference ontology the intersection of some (or all) maximal consistent subsets, whole another is to select one of the largest (in terms of number of axioms) maximal consistent subsets.

The procedure $\textsc{FindBadAxiom}(\Omc)$ may also be implemented in a number of different ways. For the evaluation we consider an implementation that randomly samples a number of (or all the) minimal inconsistent subsets of axioms $J_1, J_2, \dots, J_k \subseteq \Omc$ and selects as the bad axiom the one occurring most frequently. The implementation contains also some other implementations of $\textsc{FindBadAxiom}(\Omc)$, e.g., selecting a random axiom, selecting the axiom that appears in the least maximal consistent subsets, or selecting an axiom that does not appear in the largest (by axiom count) maximal consistent subset. Note that the implementation will consider ontologies as the union of some static axioms $\Omc_s$ and some refutable axioms $\Omc_r$ as defined in \cref{basic-definitions}. The implementation of $\textsc{FindBadAxiom}(\Omc)$ is such that it will only ever return axioms $\alpha \in \Omc_r$. To do this, minimal subsets $J$ of $\Omc_r$ such that $J \cup \Omc_s$ is inconsistent are sampled. With this feature, a variation of the repair using axiom weakening has been implemented, in which the axioms of the reference ontology $\Omcref$ are not weakened during the repair. This has been done by, after computing $\Omcref$, setting $\Omc_s \gets \Omc_s \cup \Omcref$ and $\Omc_r \gets \Omc_r \setminus \Omcref$. This variation has been evaluated in \cref{eval-quality}.

\subsection{Performance}\label{performance-impl}

The prototype implementation uses the OWL API \cite{horridge2011owl,owlapi} to represent axioms in memory and interface with off-the-shelf reasoners. The reasoners FaCT++, JFact, HermiT, and Openllet have been used in the prototype. Creating a new reasoner for each time an entailment is checked or consistency must be tested is however inefficient. One reason for this is that creating a reasoner is relatively expensive. Further, all the mentioned reasoners are, to some degree, able to keep some information computed for previous queries and exploit it to speed up subsequent ones. For these reasons, it is desirable to reuse reasoners as much as possible. To facilitate this, a class \verb|Ontology| has been created that contains a set of axioms (split into static $\Omc_s$ and refutable $\Omc_r$) as well as a reference to a set of already initialized reasoners. This class contains the methods for checking consistency and entailment, implementing them by taking one of the initialized reasoners, updating the reasoners internal state by adding and removing axioms to reflect the state of the ontologies axioms, and then calling the reasoners axioms. Only if no reasoner is available, will a new one be created and initialized. One thing this setup allows for is creating multiple ontologies that point to the same set of reasoners. To achieve this, it is recorded which ontologies use the reasoners, and if they are no longer needed, they can be disposed to free up any global resources.

To improve performance, particularly for running the test cases, running work in parallel has been considered. One problem with this approach is that all reasoners are linked to a global \verb|OWLOntologyManager| object. There is a concurrent version of this object, but not all reasoners are implemented in a way to make concurrent access to this object possible. The FaCT++ reasoner \cite{factpp} had a significantly severe problem, where it would listen to and apply all changes made to any ontology in the ontology manager. Since this manager can however contain multiple ontologies this lead to wrong reasoning results. These issues have been resolved in the slightly modified version of the FaCT++ reasoner used for this thesis.\footnote{The version of FaCT++ used for this thesis is available at \url{https://github.com/rolandbernard/factplusplus}} This version had some further modifications, like fixing an issue with the handling of annotated axioms and adding automatic loading of native dependencies, something for which previously an environment variable had to be set pointing at the dynamic library. After these changes, and some additional modifications in the prototype code, the tests can now be run concurrently. Further, the prototype contains some experiments that use multiple threads, sharing the same axiom weakening operator and cache, to compute multiple weakenings in parallel.

\subsection{Computing Minimal Subsets}\label{minimal-set-impl}



\subsection{Caching for Faster Repairs}\label{cache-impl}

The implementation will spend most of its time during repairs calling the reasoner to compute consistency and entailment queries. In order to reduce the number of calls that must be made and to accelerate the computation of upward and downward cover sets, some caching has been added. Since upward and downward covers must potentially be computed many times during a repair for a single application of the refinement or axiom weakening operators, it is an obvious point to optimize. Further, since the reference ontology and full ontology have been chosen to remain fixed for the duration of the repair, the upward and downward cover functions remain constant. When following directly the definition of cover sets presented in \cref{def:covers}, one will compute numerous subsumptions, many of which will be the same across multiple cover computations. This is therefore the aspect that caching has been applied to. The reasoners used in the implementation already have the ability to partially reuse previous results when computing queries. We therefore made sure to reuse the same reasoner instances wherever possible, to exploit this feature.

Rather than rely solely on the internal reuse of the reasoner or apply only a simple cache, however, we found it worthwhile to create a cache for subsumption, in which extra information is also inferred from the transitivity of subsumption after each query to the reasoner. This is in effect similar to the technique presented in \cite{shearer2009exploiting} for creating taxonomies, but applied also to complex concepts. Additionally, some basic rules where added to compute subsumption involving conjunctions and disjunctions, by using the results computed for the conjuncts or disjuncts of the expression. Another difference with respect to the computation of taxonomies, is that subsumptions are computed only when requested. While pre-computing the complete relation over the subconcepts would allow for ordering of the reasoner queries to maximize information gain, like one may do when computing taxonomies, this turned out to be disadvantageous, since in general a single repair will not require all results. 

\begin{algorithm}[ht]
  \begin{algorithmic}
    \State Globals $\Omc, S \gets \emptyset, K \gets \emptyset, P \gets \emptyset$ \quad (initialized once)
    \For{$X \in \{ C, D \}$ where $X \not\in S$}
      \State $S \gets S \cup \{ X \}$
      \State $K \gets K \cup \{ \langle X, X \rangle \}$
      \State $P \gets P \cup ( S \times \{ X \} ) \cup ( \{ X \} \times S )$
    \EndFor
    \If{$\langle C, D \rangle \in K$}
      \State $R \gets \mathit{True}$
    \ElsIf{$\langle C, D \rangle \not\in P$}
      \State $R \gets \mathit{False}$
    \Else
      \State $R \gets \textsc{TestSubsumtion($C, D$)}$
      \If{$R$}
        \State $K \gets K \cup \{ \langle C', D' \rangle \mid \langle C', C \rangle \in K \text{ and } \langle D, D' \rangle \in K \}$
        \State $P \gets P \setminus \{ \langle D, D' \rangle \mid \langle D', C' \rangle \in K \text{ and } \langle C, C' \rangle \not\in P \}$
        \State $P \gets P \setminus \{ \langle C', C \rangle \mid \langle D', C' \rangle \in K \text{ and } \langle D', D \rangle \not\in P \}$
      \Else
        \State $P \gets P \setminus \{ \langle C', D' \rangle \mid \langle C, C' \rangle \in K \text{ and } \langle D', D \rangle \in K \}$
      \EndIf
    \EndIf
    \State Return $R$
  \end{algorithmic}
  \caption{\textsc{CachedTestSubsumtion}($C, D$)}
  \label{algo:cached-subs}
\end{algorithm}

It follows a brief description of the algorithm used for caching subsumptions listed in \cref{algo:cached-subs}. We keep three global variables, $S$ containing all elements that have already been encountered at least once, $K$ containing the set of know tuples and $P$ the set of possible tuples. When querying a subsumption, it is first ensured, that if one or both of the elements has not yet been seen before, the appropriate tuples are inserted into $K$ and $P$. Then it is checked whether the result can be determined using the known information, and if not, the procedure \textsc{TestSubsumtion($C, D$)} is used to compute the correct result. For every new negative result, the possible tuples are reduced and for every positive result, known tuples are added, and possible tuples may also be removed. The algorithm is based on the two implications
\begin{align*}
  C \sqsubseteq_\Omc D \text{ and } D \sqsubseteq_\Omc E &\implies C \sqsubseteq_\Omc E \enspace,\text{ and} \\
  C \not\sqsubseteq_\Omc D \text{ and } C \sqsubseteq_\Omc E \text{ and } F \sqsubseteq_\Omc D &\implies E \not\sqsubseteq_\Omc F \enspace.
\end{align*}
The first implication follows trivially from the fact that subsumption between concepts or roles is transitive. The second also follows from transitivity. It can be observed that, given $C \sqsubseteq_\Omc E$ and $F \sqsubseteq_\Omc D$, if $E \sqsubseteq_\Omc F$ were to hold, then $C \sqsubseteq_\Omc D$ would also have to hold by transitivity. Separate instances of $S$, $K$ and $P$ are used for the computation of concept and role subsumptions.

For actually computing subsumptions between roles, \textsc{TestSubsumtion($C, D$)} is a simple call to the reasoner. However, for computing subsumption between concepts, one further step is used. It is applied immediately before possibly calling the reasoner. If any of the following rules can be computed using only the information in $S$, $K$, and $P$, the reasoner is not called, and the result returned from \textsc{TestSubsumtion($C, D$)}. Otherwise, the reasoner is used as usual to compute the subsumption.
\begin{align*}
  C_i \not\sqsubseteq_\Omc D \text{ for some } i \in \{ 1, \dots, n \} &\implies C_1 \sqcup \cdots \sqcup C_n \not\sqsubseteq_\Omc D \\
  C_i \sqsubseteq_\Omc D \text{ for all } i \in \{ 1, \dots, n \} &\implies C_1 \sqcup \cdots \sqcup C_n \sqsubseteq_\Omc D \\
  C \not\sqsubseteq_\Omc D_i \text{ for some } i \in \{ 1, \dots, n \} &\implies C \not\sqsubseteq_\Omc D_1 \sqcap \cdots \sqcap D_n \\
  C \sqsubseteq_\Omc D_i \text{ for all } i \in \{ 1, \dots, n \} &\implies C \sqsubseteq_\Omc D_1 \sqcap \cdots \sqcap D_n \\
  C_i \sqsubseteq_\Omc D \text{ for some } i \in \{ 1, \dots, n \} &\implies C_1 \sqcap \dots \sqcap C_n \sqsubseteq_\Omc D \\
  C \sqsubseteq_\Omc D_i \text{ for some } i \in \{ 1, \dots, n \} &\implies C \sqsubseteq_\Omc D_1 \sqcup \cdots \sqcup D_n \\
\end{align*}

\subsection{Finding Best Repairs}\label{best-repair-impl}



